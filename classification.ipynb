{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification for DRF Service Descriptions\n",
    "The object of this project is to build a NLP model to categorize text descriptions of [Data & Reasoning Fabric](https://drf.nasa.gov/) services. For now, we will be using an open-source [e-commerce dataset](https://www.kaggle.com/datasets/saurabhshahane/ecommerce-text-classification) as a placeholder for DRF service text descriptions. This dataset consists of two columns, description and label. Each description can be any of the following labels: `Household`, `Electronics`, `Books`, or `Clothing & Accessories`.\n",
    "\n",
    "To do this, we will perform cross-validation on the following machine learning classifiers: `LogisticRegression`, `KNeighborsClassifier`, `DecisionTreeClassifier`, `svm.SVC`, `SGDClassifier`, and `RidgeClassifier`. We will analyze these classifiers using BERT, tuning hyperparameters to best fit the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Paper Plane Design Framed Wall Hanging Motivat...</td>\n",
       "      <td>Household</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SAF 'Floral' Framed Painting (Wood, 30 inch x ...</td>\n",
       "      <td>Household</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SAF 'UV Textured Modern Art Print Framed' Pain...</td>\n",
       "      <td>Household</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SAF Flower Print Framed Painting (Synthetic, 1...</td>\n",
       "      <td>Household</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Incredible Gifts India Wooden Happy Birthday U...</td>\n",
       "      <td>Household</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27797</th>\n",
       "      <td>Micromax Bharat 5 Plus Zero impact on visual d...</td>\n",
       "      <td>Electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27798</th>\n",
       "      <td>Microsoft Lumia 550 8GB 4G Black Microsoft lum...</td>\n",
       "      <td>Electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27799</th>\n",
       "      <td>Microsoft Lumia 535 (Black, 8GB) Colour:Black ...</td>\n",
       "      <td>Electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27800</th>\n",
       "      <td>Karbonn Titanium Wind W4 (White) Karbonn Titan...</td>\n",
       "      <td>Electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27801</th>\n",
       "      <td>Nokia Lumia 530 (Dual SIM, Grey) Colour:Grey  ...</td>\n",
       "      <td>Electronics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27802 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             description        label\n",
       "0      Paper Plane Design Framed Wall Hanging Motivat...    Household\n",
       "1      SAF 'Floral' Framed Painting (Wood, 30 inch x ...    Household\n",
       "2      SAF 'UV Textured Modern Art Print Framed' Pain...    Household\n",
       "3      SAF Flower Print Framed Painting (Synthetic, 1...    Household\n",
       "4      Incredible Gifts India Wooden Happy Birthday U...    Household\n",
       "...                                                  ...          ...\n",
       "27797  Micromax Bharat 5 Plus Zero impact on visual d...  Electronics\n",
       "27798  Microsoft Lumia 550 8GB 4G Black Microsoft lum...  Electronics\n",
       "27799  Microsoft Lumia 535 (Black, 8GB) Colour:Black ...  Electronics\n",
       "27800  Karbonn Titanium Wind W4 (White) Karbonn Titan...  Electronics\n",
       "27801  Nokia Lumia 530 (Dual SIM, Grey) Colour:Grey  ...  Electronics\n",
       "\n",
       "[27802 rows x 2 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = \"./text_classification/ecommerceDataset.csv\"\n",
    "data = pd.read_csv(dataset, names=[\"label\", \"description\"])\n",
    "data = data[[\"description\", \"label\"]]\n",
    "\n",
    "# Clean the data - drop duplicates and missing values\n",
    "data.dropna(inplace=True)\n",
    "data.drop_duplicates(inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text normalization utils\n",
    "regexp = RegexpTokenizer(\"[\\w']+\")\n",
    "\n",
    "acronyms_url = 'https://raw.githubusercontent.com/sugatagh/E-commerce-Text-Classification/main/JSON/english_acronyms.json'\n",
    "acronyms_dict = pd.read_json(acronyms_url, typ = 'series')\n",
    "acronyms_list = list(acronyms_dict.keys())\n",
    "\n",
    "contractions_url = 'https://raw.githubusercontent.com/sugatagh/E-commerce-Text-Classification/main/JSON/english_contractions.json'\n",
    "contractions_dict = pd.read_json(contractions_url, typ = 'series')\n",
    "contractions_list = list(contractions_dict.keys())\n",
    "\n",
    "stops = stopwords.words(\"english\") # stopwords\n",
    "addstops = [\"among\", \"onto\", \"shall\", \"thrice\", \"thus\", \"twice\", \"unto\", \"us\", \"would\"] # additional stopwords\n",
    "alphabets = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"]\n",
    "prepositions = [\"about\", \"above\", \"across\", \"after\", \"against\", \"among\", \"around\", \"at\", \"before\", \"behind\", \"below\", \"beside\", \"between\", \"by\", \"down\", \"during\", \"for\", \"from\", \"in\", \"inside\", \"into\", \"near\", \"of\", \"off\", \"on\", \"out\", \"over\", \"through\", \"to\", \"toward\", \"under\", \"up\", \"with\"]\n",
    "prepositions_less_common = [\"aboard\", \"along\", \"amid\", \"as\", \"beneath\", \"beyond\", \"but\", \"concerning\", \"considering\", \"despite\", \"except\", \"following\", \"like\", \"minus\", \"onto\", \"outside\", \"per\", \"plus\", \"regarding\", \"round\", \"since\", \"than\", \"till\", \"underneath\", \"unlike\", \"until\", \"upon\", \"versus\", \"via\", \"within\", \"without\"]\n",
    "coordinating_conjunctions = [\"and\", \"but\", \"for\", \"nor\", \"or\", \"so\", \"and\", \"yet\"]\n",
    "correlative_conjunctions = [\"both\", \"and\", \"either\", \"or\", \"neither\", \"nor\", \"not\", \"only\", \"but\", \"whether\", \"or\"]\n",
    "subordinating_conjunctions = [\"after\", \"although\", \"as\", \"as if\", \"as long as\", \"as much as\", \"as soon as\", \"as though\", \"because\", \"before\", \"by the time\", \"even if\", \"even though\", \"if\", \"in order that\", \"in case\", \"in the event that\", \"lest\", \"now that\", \"once\", \"only\", \"only if\", \"provided that\", \"since\", \"so\", \"supposing\", \"that\", \"than\", \"though\", \"till\", \"unless\", \"until\", \"when\", \"whenever\", \"where\", \"whereas\", \"wherever\", \"whether or not\", \"while\"]\n",
    "others = [\"ã\", \"å\", \"ì\", \"û\", \"ûªm\", \"ûó\", \"ûò\", \"ìñ\", \"ûªre\", \"ûªve\", \"ûª\", \"ûªs\", \"ûówe\"]\n",
    "allstops = stops + addstops + alphabets + prepositions + prepositions_less_common + coordinating_conjunctions + correlative_conjunctions + subordinating_conjunctions + others\n",
    "\n",
    "spacy_lemmatizer = spacy.load(\"en_core_web_sm\", disable = ['parser', 'ner'])\n",
    "\n",
    "def remove_http(text):\n",
    "    http = \"https?://\\S+|www\\.\\S+\" # matching strings beginning with http (but not just \"http\")\n",
    "    pattern = r\"({})\".format(http) # creating pattern\n",
    "    return re.sub(pattern, \"\", text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    punct_str = string.punctuation\n",
    "    punct_str = punct_str.replace(\"'\", \"\") # discarding apostrophe from the string to keep the contractions intact\n",
    "    return text.translate(str.maketrans(\"\", \"\", punct_str))\n",
    "\n",
    "def remove_html(text):\n",
    "    html = re.compile(r'<.*?>')\n",
    "    return html.sub(r'', text)\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def convert_acronyms(text):\n",
    "    words = []\n",
    "    for word in regexp.tokenize(text):\n",
    "        if word in acronyms_list:\n",
    "            words = words + acronyms_dict[word].split()\n",
    "        else:\n",
    "            words = words + word.split()\n",
    "    \n",
    "    text_converted = \" \".join(words)\n",
    "    return text_converted\n",
    "\n",
    "def convert_contractions(text):\n",
    "    words = []\n",
    "    for word in regexp.tokenize(text):\n",
    "        if word in contractions_list:\n",
    "            words = words + contractions_dict[word].split()\n",
    "        else:\n",
    "            words = words + word.split()\n",
    "    \n",
    "    text_converted = \" \".join(words)\n",
    "    return text_converted\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in regexp.tokenize(text) if word not in allstops])\n",
    "\n",
    "def text_lemmatizer(text):\n",
    "    text_spacy = \" \".join([token.lemma_ for token in spacy_lemmatizer(text)])\n",
    "    return text_spacy\n",
    "\n",
    "def discard_non_alpha(text):\n",
    "    word_list_non_alpha = [word for word in regexp.tokenize(text) if word.isalpha()]\n",
    "    text_non_alpha = \" \".join(word_list_non_alpha)\n",
    "    return text_non_alpha\n",
    "\n",
    "def keep_pos(text):\n",
    "    tokens = regexp.tokenize(text)\n",
    "    tokens_tagged = nltk.pos_tag(tokens)\n",
    "    keep_tags = ['NN', 'NNS', 'NNP', 'NNPS', 'FW', 'PRP', 'PRPS', 'RB', 'RBR', 'RBS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WPS', 'WRB']\n",
    "    keep_words = [x[0] for x in tokens_tagged if x[1] in keep_tags]\n",
    "    return \" \".join(keep_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>paper plane design frame wall hang office deco...</td>\n",
       "      <td>Household</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>framed paint wood inch inch effect print textu...</td>\n",
       "      <td>Household</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>saf texture art print frame paint cm cm cm set...</td>\n",
       "      <td>Household</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>saf flower print frame paint inch inch texture...</td>\n",
       "      <td>Household</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gift india wooden birthday personalize gift in...</td>\n",
       "      <td>Household</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27797</th>\n",
       "      <td>micromax bharat impact displayscratch proof pr...</td>\n",
       "      <td>Electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27798</th>\n",
       "      <td>lumia gb lumia experience power window price h...</td>\n",
       "      <td>Electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27799</th>\n",
       "      <td>lumia gb colourblack product description defin...</td>\n",
       "      <td>Electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27800</th>\n",
       "      <td>karbonn titanium titanium wind performance pow...</td>\n",
       "      <td>Electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27801</th>\n",
       "      <td>lumia sim grey product description brand licen...</td>\n",
       "      <td>Electronics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27802 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             description        label\n",
       "0      paper plane design frame wall hang office deco...    Household\n",
       "1      framed paint wood inch inch effect print textu...    Household\n",
       "2      saf texture art print frame paint cm cm cm set...    Household\n",
       "3      saf flower print frame paint inch inch texture...    Household\n",
       "4      gift india wooden birthday personalize gift in...    Household\n",
       "...                                                  ...          ...\n",
       "27797  micromax bharat impact displayscratch proof pr...  Electronics\n",
       "27798  lumia gb lumia experience power window price h...  Electronics\n",
       "27799  lumia gb colourblack product description defin...  Electronics\n",
       "27800  karbonn titanium titanium wind performance pow...  Electronics\n",
       "27801  lumia sim grey product description brand licen...  Electronics\n",
       "\n",
       "[27802 rows x 2 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text normalization\n",
    "def text_normalizer(text):\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = re.sub('\\n' , '', text) # converting text to one line\n",
    "    text = re.sub('\\[.*?\\]', '', text) # removing square brackets\n",
    "    text = remove_http(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_html(text)\n",
    "    text = remove_emoji(text)\n",
    "    text = convert_acronyms(text)\n",
    "    text = convert_contractions(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = text_lemmatizer(text)\n",
    "    text = discard_non_alpha(text)\n",
    "    text = keep_pos(text)\n",
    "    return text\n",
    "\n",
    "X = data[\"description\"].apply(text_normalizer)\n",
    "y = data[\"label\"]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifiers\n",
    "n_neighbors = int(np.floor(np.sqrt(data.shape[0])))\n",
    "\n",
    "names = [\n",
    "    \"Logistic Regression\",\n",
    "    f\"KNN with {n_neighbors} neighbors\",\n",
    "    \"Decision Tree Classifier\",\n",
    "    \"Linear SVM\",\n",
    "    \"SGD Classifier\",\n",
    "    \"Ridge Classifier\"\n",
    "]\n",
    "\n",
    "models = [\n",
    "    LogisticRegression(max_iter = 1000),\n",
    "    KNeighborsClassifier(n_neighbors = n_neighbors, n_jobs = -1),\n",
    "    DecisionTreeClassifier(),\n",
    "    svm.SVC(kernel = 'linear'),\n",
    "    SGDClassifier(),\n",
    "    RidgeClassifier()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(clf, X, y, ntrials=100, test_size=0.2) :\n",
    "    train_error = 0\n",
    "    test_error = 0\n",
    "    \n",
    "    sss = StratifiedShuffleSplit(n_splits=ntrials, test_size=test_size, random_state=0)\n",
    "\n",
    "    for train_index, test_index in sss.split(X, y):\n",
    "      clf.fit(X[train_index], y[train_index])\n",
    "      y_pred_train = clf.predict(X[train_index])\n",
    "      y_pred_test = clf.predict(X[test_index])\n",
    "      train_error += 1 - accuracy_score(y[train_index], y_pred_train, normalize=True)\n",
    "      test_error += 1 - accuracy_score(y[test_index], y_pred_test, normalize=True)\n",
    "\n",
    "    train_error /= ntrials\n",
    "    test_error /= ntrials\n",
    "\n",
    "    return train_error, test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the score for each classifier\n",
    "def score(X, y, folds=5, models=models, names=names):\n",
    "    for i in range(len(models)):\n",
    "        e = error(models[i], X, y)\n",
    "        train_score = 1 - e[0]\n",
    "        test_score = 1 - e[1]\n",
    "#         scores = cross_validate(models[i], X, y, cv=folds, return_train_score=True)\n",
    "#         train_score = scores[\"train_score\"].mean()\n",
    "#         test_score = scores[\"test_score\"].mean()\n",
    "        print(names[i])\n",
    "        print(f\"\\t Train accuracy: {train_score}\")\n",
    "        print(f\"\\t Test accuracy: {test_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "\t Train accuracy: 0.9661350658693404\n",
      "\t Test accuracy: 0.9437115626685848\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "TfidfVec = TfidfVectorizer(ngram_range = (1, 1))\n",
    "X_tfidf = TfidfVec.fit_transform(X)\n",
    "score(X_tfidf, y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "https://www.capitalone.com/tech/machine-learning/understanding-tf-idf/\n",
    "\n",
    "https://towardsdatascience.com/model-selection-in-text-classification-ac13eedf6146 \n",
    "\n",
    "https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794\n",
    "\n",
    "https://www.kaggle.com/code/sugataghosh/e-commerce-text-classification-tf-idf-word2vec#Word2Vec-Model\n",
    "\n",
    "https://www.kaggle.com/code/harshjain123/bert-for-everyone-tutorial-implementation\n",
    "\n",
    "https://towardsdatascience.com/building-a-multi-label-text-classifier-using-bert-and-tensorflow-f188e0ecdc5d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
